{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Necessary imports and functions to be used, Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/typedoor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
    "\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec  \n",
    "import gensim\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the path that the data is stored. Only for local use\n",
    "PATH ='/home/typedoor/PLBRS/Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Character</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bart Simpson</th>\n",
       "      <td>19486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cartman</th>\n",
       "      <td>20593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Homer Simpson</th>\n",
       "      <td>46333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kyle</th>\n",
       "      <td>11094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lisa Simpson</th>\n",
       "      <td>15873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marge Simpson</th>\n",
       "      <td>19475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stan</th>\n",
       "      <td>11561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Line\n",
       "Character           \n",
       "Bart Simpson   19486\n",
       "Cartman        20593\n",
       "Homer Simpson  46333\n",
       "Kyle           11094\n",
       "Lisa Simpson   15873\n",
       "Marge Simpson  19475\n",
       "Stan           11561"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons = pd.read_csv(PATH + 'simpson_family_sentence_by_sentence.csv')\n",
    "#simpsons.Character = simpsons.Character.map({'Homer Simpson' : 0, 'Marge Simpson' : 1, 'Lisa Simpson' : 2, 'Bart Simpson' : 3})\n",
    "south_park = pd.read_csv(PATH + 'south_park_sentence_by_sentence.csv')\n",
    "\n",
    "merged_series = simpsons.append(south_park)\n",
    "merged_series.groupby('Character').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing steps ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to apply stemmization, lemmatization and removing stop words from data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def remove_stop_words(line):\n",
    "    result = ''\n",
    "    for word in line.lower().split():\n",
    "        if word not in stop_words:\n",
    "            result = result + \" \" + word\n",
    "    \n",
    "    return result\n",
    "\n",
    "def stemm_words(line):\n",
    "    result = ''\n",
    "    \n",
    "    for word in line.split():\n",
    "        result = result + \" \" + ps.stem(word)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def lemmatize_words(line):\n",
    "    result = ''\n",
    "    \n",
    "    for word in line.split():\n",
    "        result = result + ' ' + lem.lemmatize(word)\n",
    "     \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with the copy of the original data sets to have it as the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "south_park_processed = south_park.copy()\n",
    "simpsons_processed = simpsons.copy()\n",
    "merged_processed = merged_series.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applyting all pre-processing steps to newly copied data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "south_park_processed.Line = south_park_processed.Line.apply(remove_stop_words)\n",
    "south_park_processed.Line = south_park_processed.Line.apply(stemm_words)\n",
    "south_park_processed.Line = south_park_processed.Line.apply(lemmatize_words)\n",
    "\n",
    "simpsons_processed.Line = simpsons_processed.Line.apply(remove_stop_words)\n",
    "simpsons_processed.Line = simpsons_processed.Line.apply(stemm_words)\n",
    "simpsons_processed.Line = simpsons_processed.Line.apply(lemmatize_words)\n",
    "\n",
    "merged_processed.Line = merged_processed.Line.apply(remove_stop_words)\n",
    "merged_processed.Line = merged_processed.Line.apply(stemm_words)\n",
    "merged_processed.Line = merged_processed.Line.apply(lemmatize_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple models for classification ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data for training and test sets for each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_south = south_park_processed.Line.values\n",
    "lines_simpsons = simpsons_processed.Line.values\n",
    "lines_mer = merged_processed.Line.values\n",
    "\n",
    "y_sp = south_park_processed.Character.values\n",
    "y_sim = simpsons_processed.Character.values\n",
    "y_mer = merged_processed.Character.values\n",
    "\n",
    "\n",
    "lines_train_sp, lines_test_sp, y_train_sp, y_test_sp = train_test_split(lines_south, y_sp, test_size=0.25, random_state=21,stratify = y_sp)\n",
    "lines_train_sim, lines_test_sim, y_train_sim, y_test_sim = train_test_split(lines_simpsons, y_sim, test_size=0.25, random_state=21,stratify = y_sim)\n",
    "lines_train_mer, lines_test_mer, y_train_mer, y_test_mer = train_test_split(lines_mer, y_mer, test_size=0.25, random_state=21,stratify = y_mer)\n",
    "\n",
    "\n",
    "vectorizer_sp = CountVectorizer()\n",
    "X_train_sp =vectorizer_sp.fit_transform(lines_train_sp)\n",
    "X_test_sp = vectorizer_sp.transform(lines_test_sp)\n",
    "\n",
    "\n",
    "vectorizer_sim = CountVectorizer()\n",
    "X_train_sim = vectorizer_sim.fit_transform(lines_train_sim)\n",
    "X_test_sim  = vectorizer_sim.transform(lines_test_sim)\n",
    "\n",
    "vectorizer_mer = CountVectorizer()\n",
    "X_train_mer = vectorizer_mer.fit_transform(lines_train_mer)\n",
    "X_test_mer  = vectorizer_mer.transform(lines_test_mer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a simple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression. Unbalanced data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/typedoor/anaconda3/envs/umc/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Park 3-class classifier\n",
      "[[2854 1200 1094]\n",
      " [ 777 1158  839]\n",
      " [ 804  922 1164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Cartman       0.64      0.55      0.60      5148\n",
      "        Kyle       0.35      0.42      0.38      2774\n",
      "        Stan       0.38      0.40      0.39      2890\n",
      "\n",
      "   micro avg       0.48      0.48      0.48     10812\n",
      "   macro avg       0.46      0.46      0.46     10812\n",
      "weighted avg       0.50      0.48      0.49     10812\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/typedoor/anaconda3/envs/umc/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpsons 4-class classifier\n",
      "[[1674 1504  945  749]\n",
      " [2013 5775 1626 2169]\n",
      " [ 740 1064 1443  721]\n",
      " [ 567 1397  786 2119]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.34      0.34      0.34      4872\n",
      "Homer Simpson       0.59      0.50      0.54     11583\n",
      " Lisa Simpson       0.30      0.36      0.33      3968\n",
      "Marge Simpson       0.37      0.44      0.40      4869\n",
      "\n",
      "    micro avg       0.44      0.44      0.44     25292\n",
      "    macro avg       0.40      0.41      0.40     25292\n",
      " weighted avg       0.45      0.44      0.44     25292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/typedoor/anaconda3/envs/umc/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1383  321 1094  347  737  584  406]\n",
      " [ 410 2066  694  563  324  326  765]\n",
      " [1503  763 4785  689 1159 1800  884]\n",
      " [ 214  463  309  740  189  182  677]\n",
      " [ 606  251  776  259 1217  557  302]\n",
      " [ 416  248 1094  278  580 1936  317]\n",
      " [ 256  513  300  495  210  175  941]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.29      0.28      0.29      4872\n",
      "      Cartman       0.45      0.40      0.42      5148\n",
      "Homer Simpson       0.53      0.41      0.46     11583\n",
      "         Kyle       0.22      0.27      0.24      2774\n",
      " Lisa Simpson       0.28      0.31      0.29      3968\n",
      "Marge Simpson       0.35      0.40      0.37      4869\n",
      "         Stan       0.22      0.33      0.26      2890\n",
      "\n",
      "    micro avg       0.36      0.36      0.36     36104\n",
      "    macro avg       0.33      0.34      0.33     36104\n",
      " weighted avg       0.38      0.36      0.37     36104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='lbfgs', max_iter = 1000,class_weight = 'balanced', multi_class ='ovr')\n",
    "\n",
    "\n",
    "clf.fit(X_train_sp,y_train_sp)\n",
    "predicted = clf.predict(X_test_sp)\n",
    "print('South Park 3-class classifier')\n",
    "#print(metrics.confusion_matrix(y_test_sp, predicted))\n",
    "print(metrics.classification_report(y_test_sp, predicted))\n",
    "\n",
    "\n",
    "clf.fit(X_train_sim,y_train_sim)\n",
    "predicted = clf.predict(X_test_sim)\n",
    "print('Simpsons 4-class classifier')\n",
    "#print(metrics.confusion_matrix(y_test_sim, predicted))\n",
    "print(metrics.classification_report(y_test_sim, predicted))\n",
    "\n",
    "clf.fit(X_train_mer,y_train_mer)\n",
    "predicted = clf.predict(X_test_mer)\n",
    "print('Both Series characters')\n",
    "#print(metrics.confusion_matrix(y_test_mer, predicted))\n",
    "print(metrics.classification_report(y_test_mer, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that unbalanced data creates unbalanced predictions.\n",
    "<br>Using imbalanced learn library to make us eof oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2878 1175 1095]\n",
      " [ 740 1152  882]\n",
      " [ 854  898 1138]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Cartman       0.64      0.56      0.60      5148\n",
      "        Kyle       0.36      0.42      0.38      2774\n",
      "        Stan       0.37      0.39      0.38      2890\n",
      "\n",
      "   micro avg       0.48      0.48      0.48     10812\n",
      "   macro avg       0.46      0.46      0.45     10812\n",
      "weighted avg       0.50      0.48      0.48     10812\n",
      "\n",
      "[[1612 1450 1023  787]\n",
      " [1961 5508 1924 2190]\n",
      " [ 733 1055 1442  738]\n",
      " [ 533 1469  824 2043]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.33      0.33      0.33      4872\n",
      "Homer Simpson       0.58      0.48      0.52     11583\n",
      " Lisa Simpson       0.28      0.36      0.31      3968\n",
      "Marge Simpson       0.35      0.42      0.38      4869\n",
      "\n",
      "    micro avg       0.42      0.42      0.42     25292\n",
      "    macro avg       0.39      0.40      0.39     25292\n",
      " weighted avg       0.44      0.42      0.43     25292\n",
      "\n",
      "[[1313  403 1159  268  742  570  417]\n",
      " [ 381 2188  700  439  404  352  684]\n",
      " [1413  969 4709  562 1343 1752  835]\n",
      " [ 193  534  333  662  205  208  639]\n",
      " [ 552  318  828  203 1264  526  277]\n",
      " [ 410  312 1210  206  589 1846  296]\n",
      " [ 227  569  352  429  222  188  903]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Bart Simpson       0.29      0.27      0.28      4872\n",
      "      Cartman       0.41      0.43      0.42      5148\n",
      "Homer Simpson       0.51      0.41      0.45     11583\n",
      "         Kyle       0.24      0.24      0.24      2774\n",
      " Lisa Simpson       0.27      0.32      0.29      3968\n",
      "Marge Simpson       0.34      0.38      0.36      4869\n",
      "         Stan       0.22      0.31      0.26      2890\n",
      "\n",
      "    micro avg       0.36      0.36      0.36     36104\n",
      "    macro avg       0.33      0.34      0.33     36104\n",
      " weighted avg       0.37      0.36      0.36     36104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "clf = LogisticRegression\n",
    "\n",
    "pipeline_sp = make_pipeline(SMOTE(random_state = 42), clf(solver='lbfgs', max_iter = 1000,random_state = 42,multi_class = 'ovr'))\n",
    "model_sp = pipeline_sp.fit(X_train_sp,y_train_sp)\n",
    "predicted = model_sp.predict(X_test_sp)\n",
    "print(metrics.confusion_matrix(y_test_sp, predicted))\n",
    "print(metrics.classification_report(y_test_sp, predicted))\n",
    "\n",
    "pipeline_sim = make_pipeline(SMOTE(random_state = 42), clf(solver='lbfgs', max_iter = 1000,random_state = 42,multi_class = 'ovr'))\n",
    "model_sim = pipeline_sim.fit(X_train_sim,y_train_sim)\n",
    "predicted = model_sim.predict(X_test_sim)\n",
    "print(metrics.confusion_matrix(y_test_sim, predicted))\n",
    "print(metrics.classification_report(y_test_sim, predicted))\n",
    "\n",
    "\n",
    "pipeline_mer = make_pipeline(SMOTE(random_state = 42), clf(solver='lbfgs', max_iter = 1000,random_state = 42,multi_class = 'ovr'))\n",
    "model_mer = pipeline_mer.fit(X_train_mer,y_train_mer)\n",
    "predicted = model_mer.predict(X_test_mer)\n",
    "print(metrics.confusion_matrix(y_test_mer, predicted))\n",
    "print(metrics.classification_report(y_test_mer, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
