{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/typedoor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec  \n",
    "import gensim\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return a sentence-tokenized copy of *text*,\n",
       "using NLTK's recommended sentence tokenizer\n",
       "(currently :class:`.PunktSentenceTokenizer`\n",
       "for the specified language).\n",
       "\n",
       ":param text: text to split into sentences\n",
       ":param language: the model name in the Punkt corpus\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/umc/lib/python3.6/site-packages/nltk/tokenize/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The path tht our data is stored\n",
    "PATH = '/home/typedoor/PLBRS/Data/'\n",
    "sent_tokenize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Going away?', 'For how long?']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data, select only important characters, tokenize the corpus\n",
    "sp = pd.read_csv(PATH + 'south_park.csv')\n",
    "sp_clean = sp[(sp.Character == 'Cartman') | (sp.Character == 'Kyle')].reset_index(drop = True)\n",
    "sp_clean.Line = sp_clean.Line.apply(sent_tokenize)\n",
    "sp_clean.Line[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def remove_stop_words(line):\n",
    "    filtered_line = []\n",
    "    for word in line:\n",
    "        if word not in stop_words:\n",
    "            filtered_line.append(word)\n",
    "    \n",
    "    return filtered_line\n",
    "    #return \" \".join(filtered_line)\n",
    "\n",
    "def stemm_words(line):\n",
    "    stemmed_line = []\n",
    "    \n",
    "    for word in line:\n",
    "        stemmed_line.append(ps.stem(word))\n",
    "    \n",
    "    return stemmed_line\n",
    "    #return \"\".join(stemmed_line)\n",
    "\n",
    "def lemmatize_words(line):\n",
    "    lemmed_line = []\n",
    "    \n",
    "    for word in line:\n",
    "        lemmed_line.append(lem.lemmatize(word))\n",
    "     \n",
    "    return lemmed_line\n",
    "\n",
    "    #return \"\".join(lemmed_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing stop words, and lemmatizing\n",
    "sp_clean.Line = sp_clean.Line.apply(remove_stop_words)\n",
    "sp_clean.Line = sp_clean.Line.apply(stemm_words)\n",
    "sp_clean.Line = sp_clean.Line.apply(lemmatize_words)\n",
    "\n",
    "type(sp_clean.Line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(sp_clean['Line'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6678039145268451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Kyle       0.63      0.51      0.56      2343\n",
      "     Cartman       0.69      0.78      0.73      3226\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      5569\n",
      "   macro avg       0.66      0.65      0.65      5569\n",
      "weighted avg       0.66      0.67      0.66      5569\n",
      "\n",
      "[[1199 1144]\n",
      " [ 706 2520]]\n"
     ]
    }
   ],
   "source": [
    "y = sp_clean.Character.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, y, test_size = 0.33, random_state = 53, stratify = y)\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english',ngram_range=(1,3))\n",
    "\n",
    "#count_train = count_vectorizer.fit_transform(X_train)\n",
    "#count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(X_train,y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(score)\n",
    "\n",
    "cm = metrics.classification_report(y_test, pred,labels = ['Kyle' ,'Cartman'])\n",
    "confusion = metrics.confusion_matrix(y_test, pred, labels = ['Kyle', 'Cartman'])\n",
    "print(cm)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORD2VEC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_clean.Line = [line.lower().split() for line in sp_clean.copy().Line]\n",
    "w2v_model=Word2Vec(sp_clean.Line.values,size=20,min_count=3,window=3,sg=1,hs=0,seed=42,workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447144, 570350)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sp_clean.Line,total_examples=len(sp_clean.Line),epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15155533, -0.01953503, -0.01645868, -0.07574271, -0.02216   ,\n",
       "        0.48130372,  0.35574567, -0.20575455, -0.6798989 ,  0.8254573 ,\n",
       "       -0.31124097,  0.66434574, -0.35054132, -0.10033735, -0.01128355,\n",
       "       -0.4839375 ,  0.44111776,  0.19567357, -0.17299318,  0.42479464],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.get_vector('room')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be,', 0.9895018339157104),\n",
       " ('mind,', 0.9893526434898376),\n",
       " ('playstation', 0.9892599582672119),\n",
       " ('trick', 0.9890922904014587),\n",
       " ('happen', 0.988867998123169),\n",
       " ('alone', 0.988655686378479),\n",
       " ('belongs', 0.9883878827095032),\n",
       " ('men', 0.9882310628890991),\n",
       " ('needed', 0.9881280660629272),\n",
       " ('beating', 0.9880689978599548)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(sp_clean['Line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_tf, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.6683129197945477\n",
      "[[ 740 1397]\n",
      " [ 282 2643]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Kyle       0.72      0.35      0.47      2137\n",
      "     Cartman       0.65      0.90      0.76      2925\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      5062\n",
      "   macro avg       0.69      0.62      0.61      5062\n",
      "weighted avg       0.68      0.67      0.64      5062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "confusion = metrics.confusion_matrix(y_test, predicted, labels = ['Kyle', 'Cartman'])\n",
    "cm = metrics.classification_report(y_test, predicted,labels = ['Kyle' ,'Cartman'])\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(confusion)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.6467799288818649\n",
      "[[1614  523]\n",
      " [1265 1660]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Kyle       0.56      0.76      0.64      2137\n",
      "     Cartman       0.76      0.57      0.65      2925\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      5062\n",
      "   macro avg       0.66      0.66      0.65      5062\n",
      "weighted avg       0.68      0.65      0.65      5062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_jobs = -1, n_estimators = 7500, bootstrap = True, max_depth = 2,class_weight= 'balanced_subsample').fit(X_train,y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "confusion = metrics.confusion_matrix(y_test, predicted, labels = ['Kyle', 'Cartman'])\n",
    "cm = metrics.classification_report(y_test, predicted,labels = ['Kyle' ,'Cartman'])\n",
    "print(confusion)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/typedoor/anaconda3/envs/umc/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6977479257210588\n",
      "[[1086 1051]\n",
      " [ 479 2446]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Kyle       0.69      0.51      0.59      2137\n",
      "     Cartman       0.70      0.84      0.76      2925\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      5062\n",
      "   macro avg       0.70      0.67      0.67      5062\n",
      "weighted avg       0.70      0.70      0.69      5062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression().fit(X_train,y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "confusion = metrics.confusion_matrix(y_test, predicted, labels = ['Kyle', 'Cartman'])\n",
    "cm = metrics.classification_report(y_test, predicted,labels = ['Kyle' ,'Cartman'])\n",
    "print(confusion)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/typedoor/anaconda3/envs/umc/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'class_weight': 'balanced', 'kernel': 'linear', 'probability': True, 'shrinking': False}\n",
      "Best score is 0.6734400135466938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npredicted= clf.predict(X_test)\\nprint(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\\nconfusion = metrics.confusion_matrix(y_test, predicted, labels = [\\'Kyle\\', \\'Cartman\\'])\\ncm = metrics.classification_report(y_test, predicted,labels = [\\'Kyle\\' ,\\'Cartman\\'])\\nprint(confusion)\\nprint(cm)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "kernels = ['linear','rbf','sigmoid','poly']\n",
    "C = [0.001, 0.05, 0.1, 0.25, 0.5, 1, 1.5, 2, 3, 5]\n",
    "\n",
    "\n",
    "param_grid = {'kernel' : kernels,'shrinking' : [True, False], 'probability' : [True, False], 'class_weight' : ['balanced']}\n",
    "\n",
    "clf = SVC()\n",
    "\n",
    "svc_cv = GridSearchCV(clf,param_grid,cv = 5)\n",
    "svc_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(svc_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(svc_cv.best_score_))\n",
    "\n",
    "\n",
    "'''\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "confusion = metrics.confusion_matrix(y_test, predicted, labels = ['Kyle', 'Cartman'])\n",
    "cm = metrics.classification_report(y_test, predicted,labels = ['Kyle' ,'Cartman'])\n",
    "print(confusion)\n",
    "print(cm)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
