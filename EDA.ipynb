{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The path tht our data is stored\n",
    "PATH = '/home/typedoor/PLBRS/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Character\n",
       "Butters    2602\n",
       "Cartman    9774\n",
       "Kyle       7099\n",
       "Randy      2467\n",
       "Stan       7680\n",
       "Name: Line, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = pd.read_csv(PATH + 'south_park.csv')\n",
    "sp.groupby('Character').count().Line[sp.groupby('Character').count().Line > 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Character\n",
       "Cartman    9774\n",
       "Kyle       7099\n",
       "Name: Line, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sp_clean = sp[(sp.Character == 'Cartman') | (sp.Character == 'Kyle')| (sp.Character == \"Stan\")].reset_index(drop = True)\n",
    "sp_clean = sp[(sp.Character == 'Cartman') | (sp.Character == 'Kyle')].reset_index(drop = True)\n",
    "sp_clean.groupby('Character').count().Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sp_clean['Tokenized'] = [word_tokenize(line) for line in sp_clean.Line]\\nsp_clean.head()\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sp_clean['Tokenized'] = [word_tokenize(line) for line in sp_clean.Line]\n",
    "sp_clean.head()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def stem_words(series):\n",
    "    \n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    englishStemmer=SnowballStemmer(\"english\")\n",
    "    \n",
    "    for i in range(len(series)):\n",
    "        #print(series[i].split())\n",
    "        if i%1000 ==0:\n",
    "            print('itertions done',i)\n",
    "        result_list = [englishStemmer.stem(word) for word in series[i].split()]\n",
    "        separator = ' '\n",
    "        series[i] = separator.join(result_list)\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6796552343329143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Kyle       0.66      0.48      0.56      2343\n",
      "     Cartman       0.69      0.82      0.75      3226\n",
      "        Stan       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      5569\n",
      "   macro avg       0.45      0.43      0.44      5569\n",
      "weighted avg       0.68      0.68      0.67      5569\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/typedoor/anaconda3/envs/umc/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/typedoor/anaconda3/envs/umc/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#trying to implement how datacamp has it\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "y = sp_clean.Character.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sp_clean.Line, y, test_size = 0.33, random_state = 53, stratify = y)\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train,y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(score)\n",
    "\n",
    "cm = metrics.classification_report(y_test, pred,labels = ['Kyle' ,'Cartman','Stan'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6739091398814868\n",
      "[[ 891 1452    0]\n",
      " [ 364 2862    0]\n",
      " [   0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "#trying kinda the sme with different tokenizing\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "y = sp_clean.Character.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sp_clean.Line, y, test_size = 0.33, random_state = 53, stratify = y)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df = 0.7)\n",
    "\n",
    "\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train,y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test,pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels = ['Kyle' ,'Cartman','Stan'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.4139641 , 0.44630816, 0.13972774]]), 'Kyle')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = 'I hate jews'\n",
    "#text = tfidf_vectorizer.transform(text)\n",
    "pred = nb_classifier.predict_proba(tfidf_test[0])\n",
    "pred,y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Stan'], dtype='<U7'), array([[0.27827873, 0.28581238, 0.43590889]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"I cannot fix you\"]\n",
    "text = tfidf_vectorizer.transform(text)\n",
    "pred_prob = nb_classifier.predict_proba(text)\n",
    "pred = nb_classifier.predict(text)\n",
    "pred, pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DOING KERAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano' # Why theano why not\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 2000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = sp_clean.Line.values\n",
    "#labels = sp_clean.Character.replace({'Stan':0, 'Kyle':1, 'Cartman': 2}).values\n",
    "labels = sp_clean.Character.replace({'Kyle':0, 'Cartman': 1}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 10970\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Shape of Data Tensor: (16873, 1000)\n",
      "Shape of Label Tensor: (16873, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print(labels)\n",
    "print('Shape of Data Tensor:', data.shape)\n",
    "print('Shape of Label Tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(PATH + 'glove.6B.100d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,trainable=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 1000, 100)         1097100   \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,342,094\n",
      "Trainable params: 1,342,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12655 samples, validate on 4218 samples\n",
      "Epoch 1/10\n",
      "12655/12655 [==============================] - 291s 23ms/step - loss: 0.6377 - acc: 0.6519 - val_loss: 0.6457 - val_acc: 0.6249\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.62328 to 0.62494, saving model to model_cnn.hdf5\n",
      "Epoch 2/10\n",
      "12655/12655 [==============================] - 246s 19ms/step - loss: 0.6141 - acc: 0.6730 - val_loss: 0.6489 - val_acc: 0.6235\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.62494\n",
      "Epoch 3/10\n",
      "12655/12655 [==============================] - 289s 23ms/step - loss: 0.6116 - acc: 0.6887 - val_loss: 0.7641 - val_acc: 0.6323\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.62494 to 0.63229, saving model to model_cnn.hdf5\n",
      "Epoch 4/10\n",
      "12655/12655 [==============================] - 255s 20ms/step - loss: 0.5941 - acc: 0.7003 - val_loss: 0.7431 - val_acc: 0.6278\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.63229\n",
      "Epoch 5/10\n",
      "12655/12655 [==============================] - 246s 19ms/step - loss: 0.5803 - acc: 0.7206 - val_loss: 0.8838 - val_acc: 0.6380\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.63229 to 0.63798, saving model to model_cnn.hdf5\n",
      "Epoch 6/10\n",
      "12655/12655 [==============================] - 226s 18ms/step - loss: 0.5774 - acc: 0.7356 - val_loss: 0.9750 - val_acc: 0.6313\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.63798\n",
      "Epoch 7/10\n",
      "12655/12655 [==============================] - 231s 18ms/step - loss: 0.5428 - acc: 0.7570 - val_loss: 1.0129 - val_acc: 0.6358\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.63798\n",
      "Epoch 8/10\n",
      "12655/12655 [==============================] - 232s 18ms/step - loss: 0.5301 - acc: 0.7746 - val_loss: 1.2505 - val_acc: 0.6211\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.63798\n",
      "Epoch 9/10\n",
      "12655/12655 [==============================] - 239s 19ms/step - loss: 0.5259 - acc: 0.7889 - val_loss: 1.3790 - val_acc: 0.6183\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.63798\n",
      "Epoch 10/10\n",
      "12470/12655 [============================>.] - ETA: 3s - loss: 0.5370 - acc: 0.7983"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=10, batch_size=5,callbacks=[cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
